import os
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from llama_index.core import Document, Settings
from llama_index.readers.file import PDFReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.llms.azure_openai import AzureOpenAI
from risklab.vectorstore.llamaindex import RisklabVectorStore
from risklab.vectorstore.llamaindex.retriever import RisklabVectorStoreRetriever

# 1Ô∏è‚É£ Configure LlamaIndex + Azure credentials
Settings.embed_model = AzureOpenAIEmbedding(
    engine="text-embedding-3-small",
    azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential()),
    azure_endpoint="https://cognitiveservices.azure.com/.default",
)
Settings.llm = AzureOpenAI(
    engine="gpt-4o",
    azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential()),
    azure_endpoint="https://cognitiveservices.azure.com/.default",
)

# 2Ô∏è‚É£ Connect to existing Risklab collections (already created)
vector_store = RisklabVectorStore(
    api_key=os.getenv("RISKLAB_OPEN_AI_KEY"),
    namespace="uga-ai",
    collection_name="vector-store"
)
summary_store = RisklabVectorStore(
    api_key=os.getenv("RISKLAB_OPEN_AI_KEY"),
    namespace="uga-ai",
    collection_name="summary-store"
)
print("‚úÖ Connected to existing Risklab collections.")

# 3Ô∏è‚É£ Load and parse PDF into nodes
pdf_path = "/mnt/data/demo.pdf"  # <-- replace with your actual file
docs = PDFReader(return_full_document=True).load_data(pdf_path)
splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
nodes = splitter.get_nodes_from_documents(docs)
for node in nodes:
    node.metadata["file_name"] = os.path.basename(pdf_path)
    node.metadata["file_path"] = pdf_path

# 4Ô∏è‚É£ Embed and add document nodes to vector store
vector_store.add(Settings.embed_model(nodes))
print(f"üß† Added {len(nodes)} nodes to vector store.")

# 5Ô∏è‚É£ Generate and store summary embeddings
def summarize_document(doc: Document):
    summary_prompt = f"Summarize the following content in 5 concise sentences:\n\n{doc.text}"
    summary_text = Settings.llm.complete(summary_prompt)
    return Document(text=str(summary_text), metadata={"file_name": doc.metadata.get("file_name")})

summary_docs = [summarize_document(d) for d in docs]
summary_nodes = splitter.get_nodes_from_documents(summary_docs)
summary_store.add(Settings.embed_model(summary_nodes))
print(f"üßæ Added {len(summary_nodes)} summary nodes to summary store.")

# 6Ô∏è‚É£ Retrieve relevant content via RisklabVectorStoreRetriever
retriever = RisklabVectorStoreRetriever(vector_store=vector_store, top_k=3)
query = "What does the document say about governance in MLOps?"
results = retriever.retrieve(query)

print("\nüîç Top matches from Risklab retriever:")
for r in results:
    print(f"‚Ä¢ {r.metadata.get('file_name')} | {r.score:.3f}")
    print(r.text[:180], "...\n")

# 7Ô∏è‚É£ Safe delete routine
def delete_document(store: RisklabVectorStore, file_name: str):
    retriever = RisklabVectorStoreRetriever(vector_store=store, top_k=50)
    results = retriever.retrieve(file_name)
    deleted = 0
    for r in results:
        try:
            store.delete(ref_doc_id=r.node.id_)
            deleted += 1
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to delete node {r.node.id_}: {e}")
    print(f"üóëÔ∏è Deleted {deleted} nodes from {file_name}")

# Example deletion:
# delete_document(vector_store, os.path.basename(pdf_path))
